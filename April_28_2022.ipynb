{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "April_28_2022.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOFOQpnSMiQ9AYeyz/cosi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssh1419/NLP/blob/main/April_28_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "tH6245O6fV7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3-9. Binary cross-entropy loss"
      ],
      "metadata": {
        "id": "aDPNwCSEfaEq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da2sDuS1e6MC",
        "outputId": "0f3388b9-18b1-4a04-b00c-a54ca7b35e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2725],\n",
            "        [0.3579],\n",
            "        [0.4635],\n",
            "        [0.2568]], grad_fn=<SigmoidBackward0>)\n",
            "tensor(0.7022, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# BCELoss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "bce_loss = nn.BCELoss()\n",
        "sigmoid = nn.Sigmoid()\n",
        "probabilities = sigmoid(torch.randn(4, 1, requires_grad=True))\n",
        "targets = torch.tensor([1, 0, 1, 0],  dtype=torch.float32).view(4, 1)\n",
        "loss = bce_loss(probabilities, targets)\n",
        "print(probabilities)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diving Deep into Supervised Training\n",
        "\n",
        "### Supervised learning requires the following:\n",
        "\n",
        "\n",
        "\n",
        "*   model\n",
        "*   a loss function\n",
        "* training data\n",
        "* optimization algorithm\n",
        "\n",
        "The goal of the training is to use the gradient-based optimization algorithm to adjust the modelâ€™s parameters so that the losses are as low as possible.\n"
      ],
      "metadata": {
        "id": "vvjCawthfyGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing Toy Data\n",
        "\n",
        "\n",
        "\n",
        "*   Choosing a model\n",
        "*   Converting the probabilities to discrete classes\n",
        "* Choosing a loss function\n",
        "* Choosing an optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "eUPFo6ODgQY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3-10. Instantiating the Adam optimizer"
      ],
      "metadata": {
        "id": "0ocmoi_ngjTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "input_dim = 2\n",
        "lr = 0.001\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "    \"\"\" A perceptron is one linear layer \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): size of the input features\n",
        "        \"\"\"\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "       \n",
        "    def forward(self, x_in):\n",
        "        \"\"\"The forward pass of the perceptron\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor \n",
        "                x_in.shape should be (batch, num_features)\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,).\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.fc1(x_in)).squeeze()\n",
        "\n",
        "perceptron = Perceptron(input_dim=input_dim)\n",
        "bce_loss = nn.BCELoss()\n",
        "optimizer = optim.Adam(params=perceptron.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "6OKw0z63fnZ1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting It Together: Gradient-Based Supervised Learning\n",
        "\n",
        "\n",
        "### Example 3-11. A supervised training loop for a perceptron and binary classification"
      ],
      "metadata": {
        "id": "DJsDtPNAg64R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "# each epoch is a complete pass over the training data\n",
        "for epoch_i in range(n_epochs):\n",
        "    # the inner loop is over the batches in the dataset\n",
        "    for batch_i in range(n_batches):\n",
        "\n",
        "        # Step 0: Get the data\n",
        "        x_data, y_target = get_toy_data(batch_size)\n",
        "\n",
        "        # Step 1: Clear the gradients \n",
        "        perceptron.zero_grad()\n",
        "\n",
        "        # Step 2: Compute the forward pass of the model\n",
        "        y_pred = perceptron(x_data, apply_sigmoid=True)\n",
        "\n",
        "        # Step 3: Compute the loss value that we wish to optimize\n",
        "        loss = bce_loss(y_pred, y_target)\n",
        "\n",
        "        # Step 4: Propagate the loss signal backward\n",
        "        loss.backward()\n",
        "\n",
        "        # Step 5: Trigger the optimizer to perform one update\n",
        "        optimizer.step()\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "2Ibl8ZGYfxXG",
        "outputId": "b7550fcf-cbec-40bb-db85-b1726e6b94ef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n# each epoch is a complete pass over the training data\\nfor epoch_i in range(n_epochs):\\n    # the inner loop is over the batches in the dataset\\n    for batch_i in range(n_batches):\\n\\n        # Step 0: Get the data\\n        x_data, y_target = get_toy_data(batch_size)\\n\\n        # Step 1: Clear the gradients \\n        perceptron.zero_grad()\\n\\n        # Step 2: Compute the forward pass of the model\\n        y_pred = perceptron(x_data, apply_sigmoid=True)\\n\\n        # Step 3: Compute the loss value that we wish to optimize\\n        loss = bce_loss(y_pred, y_target)\\n\\n        # Step 4: Propagate the loss signal backward\\n        loss.backward()\\n\\n        # Step 5: Trigger the optimizer to perform one update\\n        optimizer.step()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r4zhqNn8hbdh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}